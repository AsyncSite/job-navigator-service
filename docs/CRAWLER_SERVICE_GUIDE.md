# Job Crawler Service êµ¬í˜„ ê°€ì´ë“œ

> ì´ ë¬¸ì„œëŠ” Job Crawler Serviceë¥¼ êµ¬í˜„í•˜ëŠ” ë‹¤ìŒ AIë¥¼ ìœ„í•œ ìƒì„¸ ê°€ì´ë“œì…ë‹ˆë‹¤.
> Python ê¸°ë°˜ì˜ í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤ë¥¼ Clean Architectureë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.

## ğŸ“‹ ì„ í–‰ ì¡°ê±´ í™•ì¸

### 1. Job Navigator Service ìƒíƒœ
- âœ… ëª¨ë“  API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„ ì™„ë£Œ
- âœ… ì›¹ í”„ë¡ íŠ¸ì—”ë“œì™€ ì™„ë²½í•œ ì—°ë™
- âœ… Mock ë°ì´í„°ë¡œ ì „ì²´ í”Œë¡œìš° ê²€ì¦
- âœ… ë°°ì¹˜ ì €ì¥ API ì¤€ë¹„ í•„ìš” (`POST /api/jobs/batch`)

### 2. ê°œë°œ í™˜ê²½
```bash
# Python 3.11+ í•„ìš”
python --version

# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬
cd /Users/Rene/Documents/rene/project/asyncsite
mkdir job-crawler-service
cd job-crawler-service
```

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±

```bash
# ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
mkdir -p src/adapter/{inbound/{api,scheduler},outbound/{crawler,client}}
mkdir -p src/application/{port/{inbound,outbound},service,dto}
mkdir -p src/domain
mkdir -p src/infrastructure/{config,logging,monitoring}
mkdir -p tests/{unit,integration}

# __init__.py íŒŒì¼ ìƒì„±
find src -type d -exec touch {}/__init__.py \;
```

## ğŸ”§ ê¸°ë³¸ ì„¤ì • íŒŒì¼

### 1. requirements.txt
```txt
# Core
fastapi==0.109.2
uvicorn[standard]==0.27.0
python-dotenv==1.0.0

# HTTP Client
httpx==0.26.0

# Web Scraping
beautifulsoup4==4.12.3
lxml==5.1.0

# Scheduling
apscheduler==3.10.4

# Data Validation
pydantic==2.5.3
pydantic-settings==2.1.0

# Logging
structlog==24.1.0

# Monitoring
prometheus-client==0.19.0

# Testing
pytest==7.4.4
pytest-asyncio==0.23.3
pytest-cov==4.1.0
httpx-mock==0.27.0

# Development
black==23.12.1
ruff==0.1.11
mypy==1.8.0
```

### 2. .env.example
```env
# Service Configuration
SERVICE_NAME=job-crawler-service
SERVICE_PORT=8001
ENVIRONMENT=development

# Job Navigator Service
JOB_SERVICE_URL=http://localhost:12085
JOB_SERVICE_TIMEOUT=30

# Crawler Settings
CRAWLER_USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
CRAWLER_REQUEST_DELAY_MIN=1.0
CRAWLER_REQUEST_DELAY_MAX=2.0
CRAWLER_MAX_RETRIES=3
CRAWLER_BACKOFF_FACTOR=2.0

# Scheduler Settings
SCHEDULER_ENABLED=true
SCHEDULER_CRON_HOUR=3
SCHEDULER_CRON_MINUTE=0

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# Monitoring
METRICS_ENABLED=true
METRICS_PORT=9090
```

### 3. Dockerfile
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY src/ ./src/
COPY .env.example .env

# Create non-root user
RUN useradd -m -u 1000 crawler && chown -R crawler:crawler /app
USER crawler

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=40s --retries=3 \
    CMD python -c "import httpx; httpx.get('http://localhost:8001/health')"

EXPOSE 8001

CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8001"]
```

## ğŸ“ í•µì‹¬ êµ¬í˜„ ì½”ë“œ

### 1. ë„ë©”ì¸ ëª¨ë¸
```python
# src/domain/job.py
from dataclasses import dataclass
from datetime import datetime
from typing import List, Optional
from enum import Enum

class JobType(str, Enum):
    FULLTIME = "FULLTIME"
    CONTRACT = "CONTRACT"
    INTERN = "INTERN"
    PARTTIME = "PARTTIME"

class ExperienceLevel(str, Enum):
    JUNIOR = "JUNIOR"
    SENIOR = "SENIOR"
    LEAD = "LEAD"
    ANY = "ANY"

@dataclass
class Job:
    """ì±„ìš©ê³µê³  ë„ë©”ì¸ ëª¨ë¸"""
    title: str
    company_name: str
    description: str
    requirements: str
    preferred: Optional[str]
    tech_stacks: List[str]
    job_type: JobType
    experience_level: ExperienceLevel
    location: str
    source_url: str
    posted_at: Optional[datetime]
    expires_at: Optional[datetime]
    
    def to_api_format(self) -> dict:
        """Job Navigator Service API í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return {
            "title": self.title,
            "companyName": self.company_name,
            "description": self.description,
            "requirements": self.requirements,
            "preferred": self.preferred,
            "techStackNames": self.tech_stacks,
            "jobType": self.job_type.value,
            "experienceLevel": self.experience_level.value,
            "location": self.location,
            "sourceUrl": self.source_url,
            "postedAt": self.posted_at.isoformat() if self.posted_at else None,
            "expiresAt": self.expires_at.isoformat() if self.expires_at else None
        }
```

### 2. í¬ë¡¤ëŸ¬ ë² ì´ìŠ¤ í´ë˜ìŠ¤
```python
# src/adapter/outbound/crawler/base_crawler.py
import asyncio
import random
from abc import ABC, abstractmethod
from typing import List, Optional
import httpx
from bs4 import BeautifulSoup
from structlog import get_logger

from src.domain.job import Job
from src.infrastructure.config.settings import get_settings

logger = get_logger()
settings = get_settings()

class BaseCrawler(ABC):
    """í¬ë¡¤ëŸ¬ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, company_name: str):
        self.company_name = company_name
        self.client = self._create_client()
        
    def _create_client(self) -> httpx.AsyncClient:
        """HTTP í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
        return httpx.AsyncClient(
            headers={
                'User-Agent': settings.CRAWLER_USER_AGENT,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            },
            timeout=30.0,
            follow_redirects=True
        )
    
    @abstractmethod
    async def fetch_job_list(self) -> List[str]:
        """ì±„ìš©ê³µê³  URL ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        pass
    
    @abstractmethod
    async def parse_job_detail(self, url: str) -> Optional[Job]:
        """ì±„ìš©ê³µê³  ìƒì„¸ ì •ë³´ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
        pass
    
    async def crawl(self, max_jobs: int = 50) -> List[Job]:
        """ë©”ì¸ í¬ë¡¤ë§ ë¡œì§"""
        jobs = []
        
        try:
            # 1. ì±„ìš©ê³µê³  URL ëª©ë¡ ìˆ˜ì§‘
            job_urls = await self.fetch_job_list()
            logger.info(
                "fetched_job_urls",
                company=self.company_name,
                count=len(job_urls)
            )
            
            # 2. ê° URLì— ëŒ€í•´ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
            for i, url in enumerate(job_urls[:max_jobs]):
                try:
                    # Rate limiting
                    delay = random.uniform(
                        settings.CRAWLER_REQUEST_DELAY_MIN,
                        settings.CRAWLER_REQUEST_DELAY_MAX
                    )
                    await asyncio.sleep(delay)
                    
                    # ìƒì„¸ ì •ë³´ íŒŒì‹±
                    job = await self.parse_job_detail(url)
                    if job:
                        jobs.append(job)
                        logger.info(
                            "parsed_job",
                            company=self.company_name,
                            title=job.title,
                            progress=f"{i+1}/{len(job_urls)}"
                        )
                    
                except Exception as e:
                    logger.error(
                        "parse_job_failed",
                        company=self.company_name,
                        url=url,
                        error=str(e)
                    )
                    continue
                    
        except Exception as e:
            logger.error(
                "crawl_failed",
                company=self.company_name,
                error=str(e)
            )
            raise
        finally:
            await self.client.aclose()
            
        logger.info(
            "crawl_completed",
            company=self.company_name,
            jobs_found=len(jobs)
        )
        
        return jobs
    
    async def _fetch_page(self, url: str) -> str:
        """í˜ì´ì§€ HTMLì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        response = await self.client.get(url)
        response.raise_for_status()
        return response.text
    
    def _parse_html(self, html: str) -> BeautifulSoup:
        """HTMLì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
        return BeautifulSoup(html, 'lxml')
```

### 3. ë„¤ì´ë²„ í¬ë¡¤ëŸ¬ êµ¬í˜„ ì˜ˆì‹œ
```python
# src/adapter/outbound/crawler/naver_crawler.py
from datetime import datetime
from typing import List, Optional
import re

from src.adapter.outbound.crawler.base_crawler import BaseCrawler
from src.domain.job import Job, JobType, ExperienceLevel

class NaverCrawler(BaseCrawler):
    """ë„¤ì´ë²„ ì±„ìš© í¬ë¡¤ëŸ¬"""
    
    BASE_URL = "https://recruit.navercorp.com"
    
    def __init__(self):
        super().__init__("ë„¤ì´ë²„")
    
    async def fetch_job_list(self) -> List[str]:
        """ë„¤ì´ë²„ ì±„ìš©ê³µê³  URL ëª©ë¡ ìˆ˜ì§‘"""
        job_urls = []
        
        # ê°œë°œ ì§êµ° í˜ì´ì§€
        list_url = f"{self.BASE_URL}/naver/job/list/developer"
        html = await self._fetch_page(list_url)
        soup = self._parse_html(html)
        
        # ì±„ìš©ê³µê³  ë§í¬ ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
        job_cards = soup.find_all("div", class_="card_job")
        for card in job_cards:
            link = card.find("a")
            if link and link.get("href"):
                job_url = f"{self.BASE_URL}{link['href']}"
                job_urls.append(job_url)
        
        return job_urls
    
    async def parse_job_detail(self, url: str) -> Optional[Job]:
        """ë„¤ì´ë²„ ì±„ìš©ê³µê³  ìƒì„¸ ì •ë³´ íŒŒì‹±"""
        try:
            html = await self._fetch_page(url)
            soup = self._parse_html(html)
            
            # ì œëª©
            title = soup.find("h2", class_="job_title").get_text(strip=True)
            
            # ì„¤ëª…
            description_elem = soup.find("div", class_="job_description")
            description = description_elem.get_text(strip=True) if description_elem else ""
            
            # ìê²©ìš”ê±´
            requirements_elem = soup.find("div", class_="job_requirements")
            requirements = requirements_elem.get_text(strip=True) if requirements_elem else ""
            
            # ìš°ëŒ€ì‚¬í•­
            preferred_elem = soup.find("div", class_="job_preferred")
            preferred = preferred_elem.get_text(strip=True) if preferred_elem else None
            
            # ê¸°ìˆ  ìŠ¤íƒ ì¶”ì¶œ
            tech_stacks = self._extract_tech_stacks(description + " " + requirements)
            
            # ê²½ë ¥ ìˆ˜ì¤€ íŒë‹¨
            experience_level = self._determine_experience_level(title, requirements)
            
            # ê·¼ë¬´ì§€
            location_elem = soup.find("span", class_="job_location")
            location = location_elem.get_text(strip=True) if location_elem else "ì„œìš¸"
            
            return Job(
                title=title,
                company_name=self.company_name,
                description=description,
                requirements=requirements,
                preferred=preferred,
                tech_stacks=tech_stacks,
                job_type=JobType.FULLTIME,
                experience_level=experience_level,
                location=location,
                source_url=url,
                posted_at=datetime.now(),
                expires_at=None
            )
            
        except Exception as e:
            logger.error("parse_detail_failed", url=url, error=str(e))
            return None
    
    def _extract_tech_stacks(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìˆ  ìŠ¤íƒ ì¶”ì¶œ"""
        # ì¼ë°˜ì ì¸ ê¸°ìˆ  ìŠ¤íƒ íŒ¨í„´
        tech_patterns = [
            r'\b(Java|Kotlin|Python|JavaScript|TypeScript|Go|Rust|C\+\+|C#)\b',
            r'\b(Spring|Django|FastAPI|React|Vue|Angular|Node\.js)\b',
            r'\b(MySQL|PostgreSQL|MongoDB|Redis|Elasticsearch)\b',
            r'\b(AWS|GCP|Azure|Docker|Kubernetes|Jenkins)\b',
            r'\b(Git|GitHub|GitLab|Jira|Slack)\b'
        ]
        
        tech_stacks = set()
        for pattern in tech_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            tech_stacks.update(matches)
        
        return list(tech_stacks)
    
    def _determine_experience_level(self, title: str, requirements: str) -> ExperienceLevel:
        """ê²½ë ¥ ìˆ˜ì¤€ íŒë‹¨"""
        text = f"{title} {requirements}".lower()
        
        if any(word in text for word in ["ì‹ ì…", "junior", "ì£¼ë‹ˆì–´"]):
            return ExperienceLevel.JUNIOR
        elif any(word in text for word in ["ì‹œë‹ˆì–´", "senior", "ê²½ë ¥"]):
            return ExperienceLevel.SENIOR
        elif any(word in text for word in ["ë¦¬ë“œ", "lead", "íŒ€ì¥"]):
            return ExperienceLevel.LEAD
        else:
            return ExperienceLevel.ANY
```

### 4. Job Service í´ë¼ì´ì–¸íŠ¸
```python
# src/adapter/outbound/client/job_service_client.py
import httpx
from typing import List, Dict
from structlog import get_logger

from src.domain.job import Job
from src.infrastructure.config.settings import get_settings

logger = get_logger()
settings = get_settings()

class JobServiceClient:
    """Job Navigator Service í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.base_url = settings.JOB_SERVICE_URL
        self.client = httpx.AsyncClient(
            timeout=settings.JOB_SERVICE_TIMEOUT
        )
    
    async def send_jobs(self, jobs: List[Job]) -> Dict:
        """í¬ë¡¤ë§í•œ ì±„ìš©ê³µê³ ë¥¼ Job Navigator Serviceë¡œ ì „ì†¡"""
        try:
            # Job ê°ì²´ë¥¼ API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            job_data = [job.to_api_format() for job in jobs]
            
            response = await self.client.post(
                f"{self.base_url}/api/jobs/batch",
                json=job_data
            )
            response.raise_for_status()
            
            result = response.json()
            logger.info(
                "jobs_sent",
                count=len(jobs),
                response=result
            )
            
            return result
            
        except Exception as e:
            logger.error(
                "send_jobs_failed",
                error=str(e),
                jobs_count=len(jobs)
            )
            raise
    
    async def close(self):
        """í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ"""
        await self.client.aclose()
```

### 5. í¬ë¡¤ë§ ì„œë¹„ìŠ¤
```python
# src/application/service/crawl_service.py
from typing import List, Dict
from structlog import get_logger

from src.adapter.outbound.crawler.naver_crawler import NaverCrawler
from src.adapter.outbound.crawler.kakao_crawler import KakaoCrawler
from src.adapter.outbound.crawler.woowahan_crawler import WoowahanCrawler
from src.adapter.outbound.client.job_service_client import JobServiceClient

logger = get_logger()

class CrawlService:
    """í¬ë¡¤ë§ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.crawlers = {
            "naver": NaverCrawler,
            "kakao": KakaoCrawler,
            "woowahan": WoowahanCrawler
        }
        self.job_client = JobServiceClient()
    
    async def crawl_company(self, company: str) -> Dict:
        """íŠ¹ì • íšŒì‚¬ í¬ë¡¤ë§"""
        if company not in self.crawlers:
            raise ValueError(f"Unknown company: {company}")
        
        logger.info("crawl_started", company=company)
        
        try:
            # í¬ë¡¤ëŸ¬ ì‹¤í–‰
            crawler = self.crawlers[company]()
            jobs = await crawler.crawl()
            
            # Job Serviceë¡œ ì „ì†¡
            if jobs:
                result = await self.job_client.send_jobs(jobs)
            else:
                result = {"message": "No jobs found"}
            
            logger.info(
                "crawl_completed",
                company=company,
                jobs_found=len(jobs)
            )
            
            return {
                "company": company,
                "jobs_found": len(jobs),
                "result": result
            }
            
        except Exception as e:
            logger.error(
                "crawl_failed",
                company=company,
                error=str(e)
            )
            raise
    
    async def crawl_all_companies(self) -> List[Dict]:
        """ëª¨ë“  íšŒì‚¬ í¬ë¡¤ë§"""
        results = []
        
        for company in self.crawlers.keys():
            try:
                result = await self.crawl_company(company)
                results.append(result)
            except Exception as e:
                logger.error(
                    "company_crawl_failed",
                    company=company,
                    error=str(e)
                )
                results.append({
                    "company": company,
                    "error": str(e)
                })
        
        return results
```

### 6. FastAPI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
```python
# src/main.py
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from prometheus_client import make_asgi_app
import structlog

from src.adapter.inbound.api.crawler_controller import router as crawler_router
from src.adapter.inbound.scheduler.crawl_scheduler import CrawlScheduler
from src.application.service.crawl_service import CrawlService
from src.infrastructure.config.settings import get_settings

logger = structlog.get_logger()
settings = get_settings()

# ì „ì—­ ê°ì²´
crawl_service = CrawlService()
scheduler = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    global scheduler
    
    # ì‹œì‘
    logger.info("Starting Job Crawler Service")
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
    if settings.SCHEDULER_ENABLED:
        scheduler = CrawlScheduler(crawl_service)
        scheduler.start()
        logger.info("Scheduler started")
    
    yield
    
    # ì¢…ë£Œ
    logger.info("Shutting down Job Crawler Service")
    if scheduler:
        scheduler.stop()
    await crawl_service.job_client.close()

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Job Crawler Service",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë¼ìš°í„° ë“±ë¡
app.include_router(crawler_router, prefix="/api/crawler", tags=["crawler"])

# Prometheus ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸
if settings.METRICS_ENABLED:
    metrics_app = make_asgi_app()
    app.mount("/metrics", metrics_app)

# í—¬ìŠ¤ì²´í¬
@app.get("/health")
async def health_check():
    return {"status": "healthy"}

# ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    return {
        "service": "Job Crawler Service",
        "version": "1.0.0",
        "docs": "/docs"
    }
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì „ëµ

### 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
```python
# tests/unit/test_tech_stack_extractor.py
import pytest
from src.adapter.outbound.crawler.naver_crawler import NaverCrawler

class TestTechStackExtractor:
    def test_extract_tech_stacks(self):
        crawler = NaverCrawler()
        text = "Java Spring Boot ê°œë°œì ëª¨ì§‘. React, MySQL ê²½í—˜ í•„ìˆ˜"
        
        tech_stacks = crawler._extract_tech_stacks(text)
        
        assert "Java" in tech_stacks
        assert "Spring" in tech_stacks
        assert "React" in tech_stacks
        assert "MySQL" in tech_stacks
```

### 2. í†µí•© í…ŒìŠ¤íŠ¸
```python
# tests/integration/test_crawl_service.py
import pytest
from httpx_mock import HTTPXMock

from src.application.service.crawl_service import CrawlService

@pytest.mark.asyncio
async def test_crawl_company_success(httpx_mock: HTTPXMock):
    # Mock ì„¤ì •
    httpx_mock.add_response(
        url="http://localhost:12085/api/jobs/batch",
        json={"created": 5, "updated": 2}
    )
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    service = CrawlService()
    result = await service.crawl_company("naver")
    
    assert result["company"] == "naver"
    assert "jobs_found" in result
```

## ğŸš€ ì‹¤í–‰ ë° ë°°í¬

### 1. ë¡œì»¬ ì‹¤í–‰
```bash
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
cp .env.example .env

# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ê°œë°œ ì„œë²„ ì‹¤í–‰
uvicorn src.main:app --reload --port 8001
```

### 2. Docker ì‹¤í–‰
```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t job-crawler-service .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
  --name job-crawler \
  -p 8001:8001 \
  -e JOB_SERVICE_URL=http://host.docker.internal:12085 \
  job-crawler-service
```

### 3. docker-compose í†µí•©
```yaml
# asyncsite/docker-compose.ymlì— ì¶”ê°€
job-crawler-service:
  build: ./job-crawler-service
  container_name: asyncsite-job-crawler
  ports:
    - "8001:8001"
  environment:
    - JOB_SERVICE_URL=http://job-navigator-service:12085
    - ENVIRONMENT=production
  depends_on:
    - job-navigator-service
  networks:
    - asyncsite-network
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### 1. ë¡œê·¸ í™•ì¸
```bash
# Docker ë¡œê·¸
docker logs -f job-crawler

# êµ¬ì¡°í™”ëœ ë¡œê·¸ ê²€ìƒ‰
docker logs job-crawler | jq 'select(.company == "naver")'
```

### 2. ë©”íŠ¸ë¦­ í™•ì¸
```bash
# Prometheus ë©”íŠ¸ë¦­
curl http://localhost:8001/metrics

# í¬ë¡¤ë§ í†µê³„
curl http://localhost:8001/api/crawler/stats
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. í¬ë¡¤ë§ ìœ¤ë¦¬
- robots.txt í™•ì¸ ë° ì¤€ìˆ˜
- User-Agent ëª…ì‹œ
- ìš”ì²­ ê°„ ë”œë ˆì´ (1-2ì´ˆ)
- ê³¼ë„í•œ ìš”ì²­ ê¸ˆì§€

### 2. ì—ëŸ¬ ì²˜ë¦¬
- ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ì¬ì‹œë„
- íŒŒì‹± ì˜¤ë¥˜: ë¡œê·¸ ë‚¨ê¸°ê³  ê±´ë„ˆë›°ê¸°
- êµ¬ì¡° ë³€ê²½: ì•Œë¦¼ ë°œì†¡

### 3. ë°ì´í„° ê²€ì¦
- í•„ìˆ˜ í•„ë“œ í™•ì¸
- ì¤‘ë³µ ì œê±° (source_url)
- ì´ìƒ ë°ì´í„° í•„í„°ë§

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **ì¶”ê°€ í¬ë¡¤ëŸ¬ êµ¬í˜„**
   - ì¹´ì¹´ì˜¤ í¬ë¡¤ëŸ¬
   - ì¿ íŒ¡ í¬ë¡¤ëŸ¬
   - ë°°ë‹¬ì˜ë¯¼ì¡± í¬ë¡¤ëŸ¬

2. **ê³ ë„í™”**
   - ë³‘ë ¬ í¬ë¡¤ë§
   - í”„ë¡ì‹œ ë¡œí…Œì´ì…˜
   - ìº¡ì°¨ ëŒ€ì‘

3. **ìš´ì˜ ì¤€ë¹„**
   - ì•Œë¦¼ ì‹œìŠ¤í…œ
   - ëŒ€ì‹œë³´ë“œ
   - ë°±ì—… ì „ëµ

---

**ì‘ì„±ì¼**: 2025-08-01  
**ë¬¸ì˜**: AI_BACKEND_CONTEXT.md ì°¸ì¡°